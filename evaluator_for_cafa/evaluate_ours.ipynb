{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Репозиторий оценки качества моделей методом организаторов [CAFA-evaluator](https://github.com/BioComputingUP/CAFA-evaluator)\n",
    "\n",
    "Оригинальные файлы из репозитория автора:\n",
    "\n",
    "- evaluation.py\n",
    "- graph.py\n",
    "- parser.py  \n",
    "\n",
    "\n",
    "evaluate_ours.py - запуск оценки на наших данных, завершился нехваткой памяти в  \n",
    "размере ~19 Гб."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from graph import Graph\n",
    "from parser import ia_parser, gt_parser, obo_parser, pred_parser\n",
    "from evaluation import evaluate_prediction\n",
    "\n",
    "\n",
    "# Корень проекта.\n",
    "DIR_ROOT = Path.cwd().parent.parent\n",
    "# Путь к удаленной директории с ресурсами: данные, модели и т.д.\n",
    "DIR_REMOTE: Path | None = Path('/home/admin/cafa/resources')\n",
    "\n",
    "if DIR_REMOTE is not None and DIR_REMOTE.exists():\n",
    "    DIR_RESOURCE = DIR_REMOTE\n",
    "else:\n",
    "    DIR_RESOURCE = DIR_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Собираем данные из Information accretion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input -> txt file из строк вида GO:0000003 3.27\n",
    "# Output -> dict(ключи - GO, значения - веса)\n",
    "\n",
    "ia_dict = ia_parser(DIR_RESOURCE / 'data/raw/IA.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# проверим, что во время парсинга файла, он считался целиком\n",
    "with open(DIR_RESOURCE / 'data/raw/IA.txt', \"r\") as f:\n",
    "    if len(ia_dict) == len(f.readlines()):\n",
    "        print('OK')\n",
    "    else:\n",
    "        print(\"Файл считался не целиком\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II Собираем данные из файла с онтологиями генов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input obo_parser -> obo file (go-basic.obo)\n",
    "# Output obo_parser -> dict c ключами dict_keys(\n",
    "# ['biological_process', 'molecular_function', 'cellular_component']\n",
    "# )\n",
    "# для каждого ключа значение - словарь с ключами GO и значениями -\n",
    "# параметрами этих GO\n",
    "# {\n",
    "#  'biological_process':\n",
    "#     {'GO:0000001': {'name': 'mitochondrion inheritance',\n",
    "#                     'namespace': 'biological_process',\n",
    "#                     'def': '\"The distribution of mitochondria, including the\n",
    "#                              mitochondrial genome, into daughter cells after\n",
    "#                              mitosis or meiosis, mediated by interactions\n",
    "#                              between mitochondria and the cytoskeleton.\"\n",
    "#                              [GOC:mcc, PMID:10873824, PMID:11389764]',\n",
    "#                     'alt_id': [],\n",
    "#                     'rel': ['GO:0048308', 'GO:0048311']},\n",
    "#       'GO:0000002': ... },\n",
    "#   'molecular_function': ...\n",
    "#   ...\n",
    "# }\n",
    "\n",
    "obo_file = DIR_RESOURCE / 'data/raw/Train/go-basic.obo'\n",
    "\n",
    "ontologies: list[Graph] = []\n",
    "# ns: str ('biological_process', 'molecular_function', 'cellular_component')\n",
    "# terms_dict : словарь с GO и их параметрами для каждой ns\n",
    "# ia_dict: dict(ключи - GO, значения - веса)\n",
    "# orphans=True: All terms, also those without parents\n",
    "for ns, terms_dict in obo_parser(obo_file).items():\n",
    "    ontologies.append(Graph(ns, terms_dict, ia_dict, orphans=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертировать файл tsv в txt, убрать столбец с корневыми онтологиями, чтобы\n",
    "# привести к формату авторского решения\n",
    "\n",
    "# код выполнен, файл сохранен, запускать только для нового файла ground truth\n",
    "\n",
    "# train_terms = pd.read_csv(\n",
    "#   DIR_RESOURCE / 'data/raw/Train/train_terms.tsv', sep='\\t', header=None\n",
    "# )\n",
    "\n",
    "# train_terms.drop(columns=[2], inplace=True)\n",
    "# train_terms = train_terms.iloc[1:]\n",
    "\n",
    "# train_terms.to_csv(\n",
    "#   DIR_RESOURCE / 'data/raw/Train/train_terms.txt', sep='\\t',index=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III Собираем данные по белкам, у которых известны онтологии (в нашем случае  \n",
    "это файл train_terms, test_terms есть только у организаторов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input -> gt_file, ontologies\n",
    "# Output -> dict(namespace: объект GroundTruth(ids, matrix, namespace))\n",
    "\n",
    "gt_file = DIR_RESOURCE / 'data/raw/Train/train_terms.txt'\n",
    "\n",
    "gt = gt_parser(gt_file, ontologies)  # около 4Гб RAM\n",
    "\n",
    "# Потрачено ~ 6,24 Гб Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV Формируем маccив с пороговыми значения от 0.001 до 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_step = 0.001\n",
    "\n",
    "tau_arr = np.arange(th_step, 1, th_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V Собираем массив из объектов предсказаний по каждой корневой онтологии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.2 GiB for an array with shape (92210, 27942) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m max_terms \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m \u001b[39m# количество онтологий, которое мы выбрали при обучении\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dfs \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m prediction \u001b[39m=\u001b[39m pred_parser(file_name, ontologies, gt, prop, max_terms)\n\u001b[1;32m     12\u001b[0m df_pred \u001b[39m=\u001b[39m evaluate_prediction(prediction, gt, ontologies, tau_arr, args\u001b[39m.\u001b[39mnorm, args\u001b[39m.\u001b[39mthreads)\n\u001b[1;32m     13\u001b[0m df_pred[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m file_name\n",
      "File \u001b[0;32m~/cafa/julia/kaggle_CAFA/evaluator_for_cafa/parser.py:115\u001b[0m, in \u001b[0;36mpred_parser\u001b[0;34m(pred_file, ontologies, gts, prop_mode, max_terms)\u001b[0m\n\u001b[1;32m    113\u001b[0m onts \u001b[39m=\u001b[39m {ont\u001b[39m.\u001b[39mnamespace: ont \u001b[39mfor\u001b[39;00m ont \u001b[39min\u001b[39;00m ontologies}\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m ns \u001b[39min\u001b[39;00m gts:\n\u001b[0;32m--> 115\u001b[0m     matrix[ns] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros(gts[ns]\u001b[39m.\u001b[39;49mmatrix\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    116\u001b[0m     ids[ns] \u001b[39m=\u001b[39m {}\n\u001b[1;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m onts[ns]\u001b[39m.\u001b[39mterms_dict:\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 19.2 GiB for an array with shape (92210, 27942) and data type float64"
     ]
    }
   ],
   "source": [
    "file_name = DIR_RESOURCE / 'data/submission/submission_1000.txt'\n",
    "prop = 'fill'  # by default 'max', but author choosed fill\n",
    "max_terms = 1500  # количество онтологий, которое мы выбрали при обучении\n",
    "\n",
    "# predictions: list[Prediction] - Prediction - the score matrix contains the\n",
    "# scores given by the predictor for every node of the ontology\n",
    "predictions = pred_parser(file_name, ontologies, gt, prop, max_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VI Оценка предсказаний модели разными метриками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расшифровка метрик для оценки:\n",
    "\n",
    "- tau == значение порога, который используется при формировании метрик\n",
    "'\n",
    "- cov == Coverage -> number of proteins with at least one term predicted with  \n",
    "       score >= tau\n",
    "\n",
    "- pr == precision\n",
    "\n",
    "- rc == recall\n",
    "\n",
    "- f == f1_score -> a harmonic mean of the precision and recall\n",
    "\n",
    "- wcov == Weighted coverage, compute missing and remaining uncertainty terms\n",
    "\n",
    "- wpr == Weighted precision, compute missing and remaining uncertainty terms\n",
    "\n",
    "- wrc == Weighted recall, compute missing and remaining uncertainty terms\n",
    "\n",
    "- wf == Weighted f1_score, compute missing and remaining uncertainty terms\n",
    "\n",
    "- mi == misinformation --> predicted but not in the ground truth\n",
    "\n",
    "- ru == remaining uncertainty -> not predicted but in the ground truth\n",
    "\n",
    "- s == compute np.sqrt(ru**2 + mi**2)\n",
    "\n",
    "- max_cov == метки для лучших результатов, чтобы сформировать потом df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# оценка предсказаний модели по метрикам (\"cov\", \"pr\", \"rc\", \"f\", \"wcov\",\n",
    "# \"wpr\", \"wrc\", \"wf\", \"mi\", \"ru\", \"s\", 'max_cov')\n",
    "df_pred = evaluate_prediction(predictions, gt, ontologies, tau_arr)\n",
    "df_pred['filename'] = file_name\n",
    "dfs.append(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VII Расчет средней и запись в файл результатов оценки предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На выходе получаем 1 файл csv evaluation_all.tsv a table containing the full\n",
    "# evaluation, i.e. assessment measures for each threshold. This file is used\n",
    "# as input to generate the plots (see below)\n",
    "\n",
    "# И отдельный файл для каждой метрики с лучшими строками evaluation_best_{}.tsv\n",
    "\n",
    "out_folder = DIR_RESOURCE / 'путь до папки для сохранения оценки'\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Save the dataframe\n",
    "df = df[df['cov'] > 0].reset_index(drop=True)\n",
    "df.set_index(['filename', 'ns', 'tau'], inplace=True)\n",
    "\n",
    "columns = [\n",
    "        \"cov\", \"pr\", \"rc\", \"f\", \"wcov\", \"wpr\", \"wrc\", \"wf\", \"mi\", \"ru\", \"s\"\n",
    "    ]\n",
    "\n",
    "df.to_csv(\n",
    "    '{}/evaluation_all.tsv'.format(out_folder),\n",
    "    columns=columns,\n",
    "    float_format=\"%.5f\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "# Calculate harmonic mean across namespaces for each evaluation metric\n",
    "for metric, cols in [\n",
    "    ('f', ['rc', 'pr']), ('wf', ['wrc', 'wpr']), ('s', ['ru', 'mi'])\n",
    "]:\n",
    "    if metric in columns:\n",
    "        index_best = (\n",
    "            df.groupby(level=['filename', 'ns'])[metric].idxmax()\n",
    "            if metric in ['f', 'wf']\n",
    "            else df.groupby(['filename', 'ns'])[metric].idxmin()\n",
    "            )\n",
    "\n",
    "        df_best = df.loc[index_best]\n",
    "        df_best['max_cov'] = (\n",
    "            df.reset_index('tau').loc[\n",
    "                [ele[:-1] for ele in index_best]\n",
    "                ].groupby(level=['filename', 'ns'])['cov'].max()\n",
    "            )\n",
    "        df_best.to_csv('{}/evaluation_best_{}.tsv'.format(out_folder, metric),\n",
    "                       columns=columns + [\"max_cov\"], float_format=\"%.5f\",\n",
    "                       sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
