{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008875,"end_time":"2023-05-09T08:30:09.052999","exception":false,"start_time":"2023-05-09T08:30:09.044124","status":"completed"},"tags":[]},"source":["# CAFA 5 protein function Prediction with TensorFlow\n","\n","The reference  kernel is [here](https://www.kaggle.com/code/gusthema/cafa-5-protein-function-with-tensorflow) on kaggle.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Labels of the dataset\n","\n","The objective of our model is to predict the terms (functions) of a protein sequence. One protein sequence can have many functions and can thus be classified into any number of terms. Each term is uniquely identified by a `GO Term ID`. Thus our model has to predict all the `GO Term ID`s for a protein sequence. This means that the task at hand is a multi-label classification problem. "]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008308,"end_time":"2023-05-09T08:30:09.105539","exception":false,"start_time":"2023-05-09T08:30:09.097231","status":"completed"},"tags":[]},"source":["\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008548,"end_time":"2023-05-09T08:30:09.122603","exception":false,"start_time":"2023-05-09T08:30:09.114055","status":"completed"},"tags":[]},"source":["# Protein embeddings for train and test data\n","\n","To train a machine learning model we cannot use the alphabetical protein sequences in`train_sequences.fasta` directly. They have to be converted into a vector format. In this notebook, we will use embeddings of the protein sequences to train the model. You can think of protein embeddings to be similar to word embeddings used to train NLP models.\n","<!-- Instead, to make calculations and data preparation easier we will use precalculated protein embeddings.\n"," -->\n","Protein embeddings are a machine-friendly method of capturing the protein's structural and functional characteristics, mainly through its sequence. One approach is to train a custom ML model to learn the protein embeddings of the protein sequences in the dataset being used in this notebook. Since this dataset represents proteins using amino-acid sequences which is a standard approach, we can use any publicly available pre-trained protein embedding models to generate the embeddings.\n","\n","There are a variety of protein embedding models. To make data preparation easier, we have used the precalculated protein embeddings created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model in this notebook. The precalculated protein embeddings can be found [here](https://www.kaggle.com/datasets/sergeifironov/t5embeds). We have added this dataset to the notebook along with the dataset made available for the competition.\n","\n","To add this to your enviroment, on the right side panel, click on `Add Data` and search for `t5embeds` (make sure that it's the correct [one](https://www.kaggle.com/datasets/sergeifironov/t5embeds)) and then click on the `+` beside it.\n","\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009086,"end_time":"2023-05-09T08:30:09.140473","exception":false,"start_time":"2023-05-09T08:30:09.131387","status":"completed"},"tags":[]},"source":["# Import the Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:29:53.524183Z","iopub.status.busy":"2023-08-16T13:29:53.523483Z","iopub.status.idle":"2023-08-16T13:30:06.802934Z","shell.execute_reply":"2023-08-16T13:30:06.801936Z","shell.execute_reply.started":"2023-08-16T13:29:53.52413Z"},"papermill":{"duration":9.85331,"end_time":"2023-05-09T08:30:19.002985","exception":false,"start_time":"2023-05-09T08:30:09.149675","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import gc\n","import os\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","# Uses only CPU.\n","os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n","# Changes log level.\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import f1_score\n","\n","DIR_INPUT = Path('/kaggle/input')"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:06.806456Z","iopub.status.busy":"2023-08-16T13:30:06.805335Z","iopub.status.idle":"2023-08-16T13:30:06.839773Z","shell.execute_reply":"2023-08-16T13:30:06.838683Z","shell.execute_reply.started":"2023-08-16T13:30:06.80642Z"},"trusted":true},"outputs":[],"source":["def calculating_class_weights(y_true: np.ndarray) -> np.ndarray:\n","    '''\n","    Calculation of weights for the case of 2 classes: 0, 1\n","    in multi-label classification.\n","\n","    Returns the result of the following structure:\n","    2D array with `.shape[0]` equal to the number of the targets\n","    of the multi-label classification\n","    and `.shape[1]` equal to 2:\n","    an array of two elements, where index 0 is the weight for class 0,\n","    with index 1 - weight for class 1.\n","    '''\n","    number_dim = np.shape(y_true)[1]\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        weights[i] = compute_class_weight(\n","            'balanced',\n","            classes=[0, 1],\n","            y=y_true[:, i],\n","        )\n","\n","    return weights\n","\n","\n","def f1_score_micro(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    # Classification into 2 classes with a threshold of 0.5.\n","    y_pred = np.where(y_pred >= 0.5, 1, 0)\n","    return f1_score(y_true, y_pred, average='micro')\n","\n","\n","def ia_f1_score_micro(\n","    y_true: np.ndarray,\n","    y_pred: np.ndarray,\n","    ia_arr: np.ndarray,\n",") -> float:\n","    '''\n","    Returns f1_score micro, but with ia coefficients.\n","\n","    Idea taken from the suplementary (page 31) of the paper:\n","    DOI: https://doi.org/10.1186/s13059-016-1037-6\n","    '''\n","    assert y_pred.ndim == 2 and y_true.ndim == 2 and ia_arr.ndim == 1, (\n","        'Incorrect dimension of one of the arrays:\\n'\n","        f'y_pred: {y_pred.ndim}, y_true: {y_true.ndim}, ia_arr: {ia_arr.ndim}'\n","    )\n","    assert y_pred.shape[1] == y_true.shape[1], (\n","        f'Size of y_pred ({y_pred.shape[1]}) must be equal '\n","        f'to y_true ({y_true.shape[1]})'\n","    )\n","    assert y_pred.shape[1] == ia_arr.shape[0], (\n","        f'Size y_pred ({y_pred.shape[1]}) must be equal '\n","        f'to ia_arr ({ia_arr.shape[0]})'\n","    )\n","    pred = solidify_prediction(y_pred, tau=0.5)\n","\n","    # IA TP\n","    tp_sum = (np.logical_and(pred, y_true) * ia_arr).sum()\n","    # IA pred_sum: TP + FP\n","    pred_sum = (pred * ia_arr).sum()\n","    # IA true_sum: TP + FN\n","    true_sum = (y_true * ia_arr).sum()\n","\n","    precision = tp_sum / pred_sum if pred_sum > 0 else 0\n","    recall = tp_sum / true_sum if true_sum > 0 else 0\n","\n","    # Culculatio f1_score.\n","    if precision > 0 and recall > 0:\n","        f1_score = 2 * precision * recall / (precision + recall)\n","    else:\n","        f1_score = 0\n","\n","    return f1_score\n","\n","\n","def ia_f1_score_weighted(\n","    y_true: np.ndarray,\n","    y_pred: np.ndarray,\n","    ia_arr: np.ndarray,\n","    tau: float,\n",") -> float:\n","    '''\n","    Returns f1_score when weighting and taking into account ia-coefficients.\n","\n","    Formula adapted from the suplementary (p. 31) of the paper:\n","    DOI: https://doi.org/10.1186/s13059-016-1037-6\n","\n","    [Here](https://github.com/BioComputingUP/CAFA-evaluator) you can see\n","    adaptation of this formula.\n","    '''\n","    assert y_pred.ndim == 2 and y_true.ndim == 2 and ia_arr.ndim == 1, (\n","        'Incorrect dimension of one of the arrays:\\n'\n","        f'y_pred: {y_pred.ndim}, y_true: {y_true.ndim}, ia_arr: {ia_arr.ndim}'\n","    )\n","    assert y_pred.shape[1] == y_true.shape[1], (\n","        f'Size of y_pred ({y_pred.shape[1]}) must be equal '\n","        f'to y_true ({y_true.shape[1]})'\n","    )\n","    assert y_pred.shape[1] == ia_arr.shape[0], (\n","        f'Size y_pred ({y_pred.shape[1]}) must be equal '\n","        f'to ia_arr ({ia_arr.shape[0]})'\n","    )\n","\n","    pred = solidify_prediction(y_pred, tau)\n","\n","    # Coverage, number of proteins with at least one GO-term,\n","    # predicted with score >=tau.\n","    covrg = (pred.sum(axis=1) > 0).sum()\n","\n","    # IA TP per protein\n","    n_intersection = (np.logical_and(pred, y_true) * ia_arr).sum(axis=1)\n","    # IA (TP + FP) per protein\n","    n_pred = (pred * ia_arr).sum(axis=1)\n","    # IA (TP + FN) per protein\n","    n_gt = (y_true * ia_arr).sum(axis=1)\n","\n","    # Weighted precision, recall\n","    precision = np.divide(\n","        n_intersection, n_pred,\n","        out=np.zeros_like(n_intersection, dtype='float'),\n","        where=n_pred > 0\n","    ).sum()\n","    recall = np.divide(\n","        n_intersection, n_gt,\n","        out=np.zeros_like(n_intersection, dtype='float'),\n","        where=n_gt > 0\n","    ).sum()\n","    wpr = precision / y_true.shape[0]\n","    wrc = recall / covrg if covrg > 0 else 0\n","\n","    # Weighted f1_score\n","    if wpr > 0 and wrc > 0:\n","        wf = 2 * wpr * wrc / (wpr + wrc)\n","    else:\n","        wf = 0\n","\n","    return wf\n","\n","\n","def ia_f1_score_weighted_max(\n","    y_true: np.ndarray,\n","    y_pred: np.ndarray,\n","    ia_arr: np.ndarray,\n","    tau_arr: np.ndarray,\n",") -> float:\n","    '''\n","    Returns the maximum f1_score when weighting\n","    and accounting for ia-coefficients\n","    among the `tau` parameters from `tau_arr`.\n","\n","    Idea adapted from\n","    [here](https://github.com/BioComputingUP/CAFA-evaluator).\n","    '''\n","    scores = np.zeros(len(tau_arr), dtype='float')\n","    for i, tau in enumerate(tau_arr):\n","        scores[i] = ia_f1_score_weighted(\n","            y_true,\n","            y_pred,\n","            ia_arr,\n","            tau,\n","        )\n","\n","    return np.max(scores)\n","\n","\n","def solidify_prediction(\n","    y_pred: np.ndarray,\n","    tau: float,\n",") -> np.ndarray:\n","    return y_pred >= tau\n","\n","\n","class WeightedBinaryCrossentropy(tf.keras.losses.Loss):\n","    '''\n","    Calculates the weighted `tf.keras.losses.BinaryCrossentropy`.\n","    See constructor parameters for more information\n","    in the `tf.keras.losses.BinaryCrossentropy` documentation.\n","\n","    `class_weights`: None | np.ndarray\n","    - In the case of `None`, the results of the calculation without weighting are returned.\n","    - In the case of `np.ndarray`, the structure should be:\n","    2D array with `.shape[0]` equal to the number of targets of the multi-label classification\n","    and `.shape[1]` equal to 2:\n","    an array of two elements, where index 0 is the weight for class 0,\n","    with index 1 - weight for class 1.\n","    '''\n","    def __init__(\n","        self,\n","        class_weights: None | np.ndarray = None,\n","        from_logits: bool = False,\n","        label_smoothing: float = 0.0,\n","        name: str | None = 'weighted_binary_crossentropy',\n","        axis=-1,\n","    ):\n","        # The case when the loss function is reduced to binary_crossentropy.\n","        if class_weights is None or np.all(class_weights == 1.0):\n","            self.class_weights = None\n","            name = 'binary_crossentropy'\n","        else:\n","            self.class_weights = tf.convert_to_tensor(\n","                class_weights,\n","                dtype=tf.float32\n","            )\n","        self.default_bce = tf.keras.losses.BinaryCrossentropy(\n","            from_logits=from_logits,\n","            label_smoothing=label_smoothing,\n","            axis=axis,\n","        )\n","        super(WeightedBinaryCrossentropy, self).__init__(name=name)\n","\n","    def __call__(self, y_true, y_pred, sample_weight=None):\n","        loss = self.default_bce(y_true, y_pred, sample_weight)\n","        if self.class_weights is not None:\n","            y_true = tf.cast(y_true, tf.float32)\n","            loss = K.mean(\n","                (\n","                    (self.class_weights[:, 0]**(1-y_true))  # type: ignore\n","                    * (self.class_weights[:, 1]**(y_true))  # type: ignore\n","                    * self.default_bce(y_true, y_pred)\n","                ),\n","                axis=-1\n","            )\n","\n","        return loss\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008429,"end_time":"2023-05-09T08:30:19.047756","exception":false,"start_time":"2023-05-09T08:30:19.039327","status":"completed"},"tags":[]},"source":["# Load the Dataset"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008388,"end_time":"2023-05-09T08:30:19.065367","exception":false,"start_time":"2023-05-09T08:30:19.056979","status":"completed"},"tags":[]},"source":["First we will load the file `train_terms.tsv` which contains the list of annotated terms (functions) for the proteins. We will extract the labels aka `GO term ID` and create a label dataframe for the protein embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:06.841584Z","iopub.status.busy":"2023-08-16T13:30:06.841168Z","iopub.status.idle":"2023-08-16T13:30:11.092211Z","shell.execute_reply":"2023-08-16T13:30:11.090733Z","shell.execute_reply.started":"2023-08-16T13:30:06.841556Z"},"papermill":{"duration":3.69155,"end_time":"2023-05-09T08:30:22.766144","exception":false,"start_time":"2023-05-09T08:30:19.074594","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_terms = pd.read_csv(\n","    DIR_INPUT / 'cafa-5-protein-function-prediction/Train/train_terms.tsv',\n","    sep='\\t'\n",")\n","print(train_terms.shape)\n","print(round(train_terms.memory_usage().sum() / 1024**3, 2), 'GB')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008358,"end_time":"2023-05-09T08:30:22.783293","exception":false,"start_time":"2023-05-09T08:30:22.774935","status":"completed"},"tags":[]},"source":["`train_terms` dataframe is composed of 3 columns and 5_363_863 entries. We can see all 3 dimensions of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:11.097623Z","iopub.status.busy":"2023-08-16T13:30:11.09683Z","iopub.status.idle":"2023-08-16T13:30:11.129328Z","shell.execute_reply":"2023-08-16T13:30:11.12782Z","shell.execute_reply.started":"2023-08-16T13:30:11.097579Z"},"papermill":{"duration":0.038607,"end_time":"2023-05-09T08:30:22.830633","exception":false,"start_time":"2023-05-09T08:30:22.792026","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_terms.head()"]},{"cell_type":"markdown","metadata":{},"source":["## IA-coefficients"]},{"cell_type":"markdown","metadata":{},"source":["### Load IA data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:11.13363Z","iopub.status.busy":"2023-08-16T13:30:11.1327Z","iopub.status.idle":"2023-08-16T13:30:11.198763Z","shell.execute_reply":"2023-08-16T13:30:11.197396Z","shell.execute_reply.started":"2023-08-16T13:30:11.133588Z"},"trusted":true},"outputs":[],"source":["ia_df = pd.read_csv(\n","    DIR_INPUT / 'cafa-5-protein-function-prediction/IA.txt',\n","    names=['term', 'ia'],\n","    sep=\"\\t\",\n",")\n","print(f'IA data shape: {ia_df.shape}')\n","ia_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Add IA-coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:11.200633Z","iopub.status.busy":"2023-08-16T13:30:11.20023Z","iopub.status.idle":"2023-08-16T13:30:15.124651Z","shell.execute_reply":"2023-08-16T13:30:15.123209Z","shell.execute_reply.started":"2023-08-16T13:30:11.2006Z"},"trusted":true},"outputs":[],"source":["train_terms = pd.merge(\n","    train_terms,\n","    ia_df,\n","    on='term',\n","    how='inner',\n",")\n","print(train_terms.shape)\n","train_terms.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the protein embeddings\n","\n","\n","We will now load the pre calculated protein embeddings created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model.\n","\n","If the `tfembeds` is not yet on the input data of the notebook, you can add it to your enviromentby clicking on `Add Data` and search for `t5embeds` (make sure that it's the correct [one](https://www.kaggle.com/datasets/sergeifironov/t5embeds) ) and then click on the `+` beside it.\n","\n","The protein embeddings to be used for training are recorded in `train_embeds.npy` and the corresponding protein ids are available in `train_ids.npy`."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009256,"end_time":"2023-05-09T08:30:22.867158","exception":false,"start_time":"2023-05-09T08:30:22.857902","status":"completed"},"tags":[]},"source":["First, we will load the protein ids of the protein embeddings in the train dataset contained in `train_ids.npy` into a numpy array."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:15.127413Z","iopub.status.busy":"2023-08-16T13:30:15.126623Z","iopub.status.idle":"2023-08-16T13:30:15.188489Z","shell.execute_reply":"2023-08-16T13:30:15.187225Z","shell.execute_reply.started":"2023-08-16T13:30:15.127369Z"},"papermill":{"duration":0.067806,"end_time":"2023-05-09T08:30:22.944355","exception":false,"start_time":"2023-05-09T08:30:22.876549","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_protein_ids = np.load(\n","    DIR_INPUT / 't5embeds/train_ids.npy'\n",")\n","print(train_protein_ids.shape)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009498,"end_time":"2023-05-09T08:30:22.963291","exception":false,"start_time":"2023-05-09T08:30:22.953793","status":"completed"},"tags":[]},"source":["The `train_protein_ids` array consists of 142246 protein_ids. Let us print out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:15.191182Z","iopub.status.busy":"2023-08-16T13:30:15.190699Z","iopub.status.idle":"2023-08-16T13:30:15.199246Z","shell.execute_reply":"2023-08-16T13:30:15.197915Z","shell.execute_reply.started":"2023-08-16T13:30:15.191143Z"},"papermill":{"duration":0.019907,"end_time":"2023-05-09T08:30:22.992625","exception":false,"start_time":"2023-05-09T08:30:22.972718","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_protein_ids[:5]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009375,"end_time":"2023-05-09T08:30:23.011402","exception":false,"start_time":"2023-05-09T08:30:23.002027","status":"completed"},"tags":[]},"source":["<!-- Now, we will load`train_embeds.py` which contains the pre-calculated embeddings of the proteins in the train dataset. with protein_ids (`id`s we loaded previously from the **train_ids.npy**) into a numpy array. This array now contains the precalculated embeddings for the protein_ids( Ids we loaded above from **train_ids.npy**) needed for training. -->\n","\n","After loading the files as numpy arrays, we will convert them into Pandas dataframe.\n","\n","Each protein embedding is a vector of length 1024. We create the resulting dataframe such that there are 1024 columns to represent the values in each of the 1024 places in the vector."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:15.202442Z","iopub.status.busy":"2023-08-16T13:30:15.201574Z","iopub.status.idle":"2023-08-16T13:30:27.065693Z","shell.execute_reply":"2023-08-16T13:30:27.064143Z","shell.execute_reply.started":"2023-08-16T13:30:15.202398Z"},"papermill":{"duration":9.719957,"end_time":"2023-05-09T08:30:32.741095","exception":false,"start_time":"2023-05-09T08:30:23.021138","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_embeddings = np.load(\n","    DIR_INPUT / 't5embeds/train_embeds.npy'\n",")\n","\n","# Now lets convert embeddings numpy array(train_embeddings)\n","# into pandas dataframe.\n","column_num = train_embeddings.shape[1]\n","train_df = pd.DataFrame(\n","    train_embeddings,\n","    columns = [\"Column_\" + str(i) for i in range(1, column_num+1)]\n",")\n","# Removes a redundant  object.\n","del train_embeddings\n","print(train_df.shape)\n","print(round(train_df.memory_usage().sum() / 1024**3, 2), 'GB')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00918,"end_time":"2023-05-09T08:30:32.760375","exception":false,"start_time":"2023-05-09T08:30:32.751195","status":"completed"},"tags":[]},"source":["The `train_df` dataframe which contains the embeddings is composed of 1024 columns and 142246 entries. We can see all 1024 dimensions(results will be truncated since column length is too long)  of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:27.071434Z","iopub.status.busy":"2023-08-16T13:30:27.071004Z","iopub.status.idle":"2023-08-16T13:30:27.106365Z","shell.execute_reply":"2023-08-16T13:30:27.10483Z","shell.execute_reply.started":"2023-08-16T13:30:27.071401Z"},"papermill":{"duration":0.036828,"end_time":"2023-05-09T08:30:32.807222","exception":false,"start_time":"2023-05-09T08:30:32.770394","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009208,"end_time":"2023-05-09T08:30:32.825978","exception":false,"start_time":"2023-05-09T08:30:32.81677","status":"completed"},"tags":[]},"source":["# Prepare the dataset\n","\n","Reference: https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009674,"end_time":"2023-05-09T08:30:32.845065","exception":false,"start_time":"2023-05-09T08:30:32.835391","status":"completed"},"tags":[]},"source":["First we will extract all the needed labels(`GO term ID`) from `train_terms.tsv` file. There are more than 40,000 labels. In order to simplify our model, we will choose the most frequent 1500 `GO term ID`s as labels."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009238,"end_time":"2023-05-09T08:30:32.863785","exception":false,"start_time":"2023-05-09T08:30:32.854547","status":"completed"},"tags":[]},"source":["Let's plot the most frequent 100 `GO Term ID`s in `train_terms.tsv`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:27.110473Z","iopub.status.busy":"2023-08-16T13:30:27.109889Z","iopub.status.idle":"2023-08-16T13:30:44.152368Z","shell.execute_reply":"2023-08-16T13:30:44.150866Z","shell.execute_reply.started":"2023-08-16T13:30:27.110427Z"},"papermill":{"duration":1.592489,"end_time":"2023-05-09T08:30:34.465912","exception":false,"start_time":"2023-05-09T08:30:32.873423","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Select first 100 values for plotting.\n","plot_df = train_terms['term'].value_counts().iloc[:1500]\n","\n","figure, axis = plt.subplots(1, 1, figsize=(12, 6))\n","\n","bp = sns.barplot(ax=axis, x=np.array(plot_df.index), y=plot_df.values)\n","bp.set_xticklabels(bp.get_xticklabels(), rotation=90, size = 6)\n","axis.set_title('Top 100 frequent GO term IDs')\n","bp.set_xlabel('GO term IDs', fontsize = 12)\n","bp.set_ylabel('Count', fontsize = 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010458,"end_time":"2023-05-09T08:30:34.487707","exception":false,"start_time":"2023-05-09T08:30:34.477249","status":"completed"},"tags":[]},"source":["We will now save the first 1500 most frequent GO term Ids into a list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:44.154604Z","iopub.status.busy":"2023-08-16T13:30:44.154228Z","iopub.status.idle":"2023-08-16T13:30:45.277111Z","shell.execute_reply":"2023-08-16T13:30:45.275671Z","shell.execute_reply.started":"2023-08-16T13:30:44.154573Z"},"papermill":{"duration":0.523976,"end_time":"2023-05-09T08:30:35.021974","exception":false,"start_time":"2023-05-09T08:30:34.497998","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Set the limit for label.\n","num_of_labels = 1500\n","\n","# Take value counts in descending order\n","# and fetch first 1500 `GO term ID` as labels\n","labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()\n","# IA-coefficients array coresponding to the `labels`."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009833,"end_time":"2023-05-09T08:30:35.042088","exception":false,"start_time":"2023-05-09T08:30:35.032255","status":"completed"},"tags":[]},"source":["Next, we will create a new dataframe by filtering the train terms with the selected `GO Term ID`s."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:45.279772Z","iopub.status.busy":"2023-08-16T13:30:45.279366Z","iopub.status.idle":"2023-08-16T13:30:46.178233Z","shell.execute_reply":"2023-08-16T13:30:46.176666Z","shell.execute_reply.started":"2023-08-16T13:30:45.279739Z"},"papermill":{"duration":0.668657,"end_time":"2023-05-09T08:30:35.720953","exception":false,"start_time":"2023-05-09T08:30:35.052296","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Fetch the train_terms data for the relevant labels only\n","train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]\n","# Intermediate dataframe for the formation of a single indexing\n","# for \"term\" and \"ia\".\n","term_ia_df = (\n","    train_terms_updated[['term', 'ia']]\n","    .drop_duplicates()\n","    .reset_index(drop=True)\n",")\n","# Rewrite `labels` with posible a new order of the GO-terms.\n","labels = term_ia_df['term'].tolist()\n","# AI-coef array corresponding to `labels`.\n","ia_arr = term_ia_df['ia'].to_numpy()\n","# Removes redundant objects.\n","del train_terms"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009797,"end_time":"2023-05-09T08:30:35.741328","exception":false,"start_time":"2023-05-09T08:30:35.731531","status":"completed"},"tags":[]},"source":["Let us plot the aspect values in the new **train_terms_updated** dataframe using a pie chart."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:46.180608Z","iopub.status.busy":"2023-08-16T13:30:46.180162Z","iopub.status.idle":"2023-08-16T13:30:47.048806Z","shell.execute_reply":"2023-08-16T13:30:47.047031Z","shell.execute_reply.started":"2023-08-16T13:30:46.180562Z"},"papermill":{"duration":0.419624,"end_time":"2023-05-09T08:30:36.171193","exception":false,"start_time":"2023-05-09T08:30:35.751569","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["pie_df = train_terms_updated['aspect'].value_counts()\n","palette_color = sns.color_palette('bright')\n","plt.pie(pie_df.values, labels=np.array(pie_df.index), colors=palette_color, autopct='%.0f%%')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016642,"end_time":"2023-05-09T08:30:36.204943","exception":false,"start_time":"2023-05-09T08:30:36.188301","status":"completed"},"tags":[]},"source":["As you can see, majority of the `GO term Id`s have BPO(Biological Process Ontology) as their aspect."]},{"cell_type":"markdown","metadata":{},"source":["Since this is a multi label classification problem, in the labels array we will denote the presence or absence of each Go Term Id for a protein id using a 1 or 0.\n","First, we will create a numpy array `train_labels` of required size for the labels. To update the `train_labels` array with the appropriate values, we will loop through the label list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:30:47.051643Z","iopub.status.busy":"2023-08-16T13:30:47.051101Z","iopub.status.idle":"2023-08-16T13:51:29.30605Z","shell.execute_reply":"2023-08-16T13:51:29.304714Z","shell.execute_reply.started":"2023-08-16T13:30:47.051597Z"},"papermill":{"duration":495.729474,"end_time":"2023-05-09T08:38:51.951408","exception":false,"start_time":"2023-05-09T08:30:36.221934","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Create an empty dataframe of required size for storing the labels,\n","# i.e, train_size x num_of_labels (142246 x 1500)\n","train_size = train_protein_ids.shape[0] # len(X)\n","train_labels = np.zeros((train_size ,num_of_labels))\n","\n","# Convert from numpy to pandas series for better handling\n","series_train_protein_ids = pd.Series(train_protein_ids)\n","\n","# Loop through each label\n","for i in range(num_of_labels):\n","    # For each label, fetch the corresponding train_terms data\n","    n_train_terms = train_terms_updated[\n","        train_terms_updated['term'] ==  labels[i]\n","    ]\n","    \n","    # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n","    label_related_proteins = n_train_terms['EntryID'].unique()\n","    \n","    # In the series_train_protein_ids pandas series, if a protein is related\n","    # to the current label, then mark it as 1, else 0.\n","    # Replace the ith column of train_Y with with that pandas series.\n","    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n","    \n","# Convert train_Y numpy into pandas dataframe\n","labels_df = pd.DataFrame(data = train_labels, columns = labels)\n","labels_df = labels_df.astype('int32')\n","print(labels_df.shape)\n","print(round(labels_df.memory_usage().sum() / 1024**3, 2), 'GB')\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010097,"end_time":"2023-05-09T08:38:51.971947","exception":false,"start_time":"2023-05-09T08:38:51.96185","status":"completed"},"tags":[]},"source":["The final labels dataframe (`label_df`) is composed of 1500 columns and 142_246 entries. We can see all 1500 dimensions(results will be truncated since the number of columns is big) of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:51:29.308346Z","iopub.status.busy":"2023-08-16T13:51:29.307828Z","iopub.status.idle":"2023-08-16T13:51:29.335243Z","shell.execute_reply":"2023-08-16T13:51:29.332571Z","shell.execute_reply.started":"2023-08-16T13:51:29.308285Z"},"papermill":{"duration":0.048128,"end_time":"2023-05-09T08:38:52.031041","exception":false,"start_time":"2023-05-09T08:38:51.982913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["labels_df.head()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010523,"end_time":"2023-05-09T08:38:52.052433","exception":false,"start_time":"2023-05-09T08:38:52.04191","status":"completed"},"tags":[]},"source":["# Training\n","\n","Next, we will use Tensorflow to train a Deep Neural Network with the protein embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-16T13:51:29.338721Z","iopub.status.busy":"2023-08-16T13:51:29.337831Z","iopub.status.idle":"2023-08-16T13:52:35.08337Z","shell.execute_reply":"2023-08-16T13:52:35.081068Z","shell.execute_reply.started":"2023-08-16T13:51:29.338664Z"},"papermill":{"duration":128.96621,"end_time":"2023-05-09T08:41:01.029422","exception":false,"start_time":"2023-05-09T08:38:52.063212","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Params list:\n","ACTIVATION = 'relu'\n","BATCH_SIZE = 5000\n","CALLBACKS = [\n","    tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        mode='min',\n","        min_delta=0.01,\n","        patience=10,\n","        restore_best_weights=True,\n","    ),\n","]\n","DROPOUT = 0.1\n","EPOCHS = 39\n","KERNEL_REGULARIZER = None\n","# The structure of hidden layers in a serial DNN.\n","LAYERS_STRCT = [2048, 1024, 1024]\n","LEARNING_RATE = 0.001\n","LOSS = WeightedBinaryCrossentropy(\n","    # calculating_class_weights(y_train.to_numpy())\n",")\n","METRIC_TH_STEP = 0.01\n","METRIC_TAU = 0.5\n","N_INPUTS = train_df.shape[1]\n","N_OUTPUTS = labels_df.shape[1]\n","SHUFFLE = True\n","VERBOSE = 1\n","\n","# --- Create DNN.\n","# Input layer.\n","inputs = tf.keras.Input(shape=N_INPUTS)\n","batch_normalization_lr = tf.keras.layers.BatchNormalization()(inputs)\n","# Creating hiden layers of DNN.\n","x = batch_normalization_lr\n","for units in LAYERS_STRCT:\n","    x = tf.keras.layers.Dense(\n","        units=units,\n","        activation=ACTIVATION,\n","        kernel_regularizer=KERNEL_REGULARIZER,\n","    )(x)\n","    if DROPOUT > 0:\n","        x = tf.keras.layers.Dropout(DROPOUT)(x)\n","# Output layer.\n","outputs = tf.keras.layers.Dense(N_OUTPUTS, activation='sigmoid')(x)\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","# --- Model compilation.\n","model.compile(\n","    loss=LOSS,\n","    optimizer=tf.keras.optimizers.Adam(\n","        learning_rate=LEARNING_RATE,\n","    )\n",")\n","\n","# Train model.\n","history = model.fit(\n","    train_df, labels_df,\n","    batch_size=BATCH_SIZE,\n","    epochs=EPOCHS,\n",")\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.021665,"end_time":"2023-05-09T08:41:01.780867","exception":false,"start_time":"2023-05-09T08:41:01.759202","status":"completed"},"tags":[]},"source":["# Submission"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.02075,"end_time":"2023-05-09T08:41:01.82296","exception":false,"start_time":"2023-05-09T08:41:01.80221","status":"completed"},"tags":[]},"source":["For submission we will use the protein embeddings of the test data created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:52:35.086736Z","iopub.status.idle":"2023-08-16T13:52:35.087331Z","shell.execute_reply":"2023-08-16T13:52:35.087041Z","shell.execute_reply.started":"2023-08-16T13:52:35.087015Z"},"papermill":{"duration":10.290827,"end_time":"2023-05-09T08:41:12.134919","exception":false,"start_time":"2023-05-09T08:41:01.844092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_embeddings = np.load(\n","    DIR_INPUT / 't5embeds/test_embeds.npy'\n",")\n","\n","# Convert test_embeddings to dataframe\n","column_num = test_embeddings.shape[1]\n","test_df = pd.DataFrame(test_embeddings, columns = [\"Column_\" + str(i) for i in range(1, column_num+1)])\n","print(test_df.shape)\n","print(test_df.memory_usage().sum() / 1024**3, 'GB')\n","print(round(test_df.memory_usage().sum() / 1024**3, 2), 'GB')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.020857,"end_time":"2023-05-09T08:41:12.17776","exception":false,"start_time":"2023-05-09T08:41:12.156903","status":"completed"},"tags":[]},"source":["The `test_df` is composed of 1024 columns and 141865 entries. We can see all 1024 dimensions(results will be truncated since column length is too long) of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:52:35.090333Z","iopub.status.idle":"2023-08-16T13:52:35.091174Z","shell.execute_reply":"2023-08-16T13:52:35.09095Z","shell.execute_reply.started":"2023-08-16T13:52:35.090923Z"},"papermill":{"duration":0.050123,"end_time":"2023-05-09T08:41:12.248732","exception":false,"start_time":"2023-05-09T08:41:12.198609","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["We will now use the model to make predictions on the test embeddings. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:52:35.092614Z","iopub.status.idle":"2023-08-16T13:52:35.093072Z","shell.execute_reply":"2023-08-16T13:52:35.092874Z","shell.execute_reply.started":"2023-08-16T13:52:35.092854Z"},"papermill":{"duration":663.907351,"end_time":"2023-05-09T08:52:16.178461","exception":false,"start_time":"2023-05-09T08:41:12.27111","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["predictions = model.predict(test_df)\n","# Removes a redundant  object.\n","del test_df\n","print(f'`predictions` shape: {predictions.shape}')\n","# Rounding of probabilities\n","# according to the requirements of the competition rules.\n","predictions = predictions.round(3)\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:52:35.095792Z","iopub.status.idle":"2023-08-16T13:52:35.096301Z","shell.execute_reply":"2023-08-16T13:52:35.096093Z","shell.execute_reply.started":"2023-08-16T13:52:35.096069Z"},"trusted":true},"outputs":[],"source":["# Loads protein IDs from embedded test data.\n","test_protein_ids = np.load(\n","    DIR_INPUT / 't5embeds/test_ids.npy'\n",")\n","assert test_protein_ids.shape[0] == predictions.shape[0], (\n","    'Number protein ids in embedded list must be equal to proteins number in predictions.'\n",")\n","test_protein_ids"]},{"cell_type":"markdown","metadata":{},"source":["Creates from `predictions` `submission.tsv` file in streaming writing manner."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-08-16T13:52:35.09995Z","iopub.status.idle":"2023-08-16T13:52:35.100577Z","shell.execute_reply":"2023-08-16T13:52:35.100351Z","shell.execute_reply.started":"2023-08-16T13:52:35.100301Z"},"papermill":{"duration":0.063739,"end_time":"2023-05-09T08:52:16.292974","exception":false,"start_time":"2023-05-09T08:52:16.229235","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["submission_chunk_df = pd.DataFrame(\n","    columns = ['Protein Id', 'GO Term Id','Prediction']\n",")\n","for prot_idx, prot_id in enumerate(test_protein_ids, 0):\n","    prot_chunk = [prot_id] * predictions.shape[1]\n","\n","    submission_chunk_df['Protein Id'] = [prot_id] * predictions.shape[1]\n","    submission_chunk_df['GO Term Id'] = labels\n","    submission_chunk_df['Prediction'] = predictions[prot_idx]\n","\n","    submission_chunk_df.to_csv(\n","        '/kaggle/working/submission.tsv',\n","        header=False,\n","        index=False,\n","        mode='a',\n","        sep='\\t'\n","    )\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
