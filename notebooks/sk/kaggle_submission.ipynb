{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008875,"end_time":"2023-05-09T08:30:09.052999","exception":false,"start_time":"2023-05-09T08:30:09.044124","status":"completed"},"tags":[]},"source":["# CAFA 5 protein function Prediction with TensorFlow\n","\n","The reference  kernel is [here](https://www.kaggle.com/code/gusthema/cafa-5-protein-function-with-tensorflow) on kaggle.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Labels of the dataset\n","\n","The objective of our model is to predict the terms (functions) of a protein sequence. One protein sequence can have many functions and can thus be classified into any number of terms. Each term is uniquely identified by a `GO Term ID`. Thus our model has to predict all the `GO Term ID`s for a protein sequence. This means that the task at hand is a multi-label classification problem. "]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008308,"end_time":"2023-05-09T08:30:09.105539","exception":false,"start_time":"2023-05-09T08:30:09.097231","status":"completed"},"tags":[]},"source":["\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008548,"end_time":"2023-05-09T08:30:09.122603","exception":false,"start_time":"2023-05-09T08:30:09.114055","status":"completed"},"tags":[]},"source":["# Protein embeddings for train and test data\n","\n","To train a machine learning model we cannot use the alphabetical protein sequences in`train_sequences.fasta` directly. They have to be converted into a vector format. In this notebook, we will use embeddings of the protein sequences to train the model. You can think of protein embeddings to be similar to word embeddings used to train NLP models.\n","<!-- Instead, to make calculations and data preparation easier we will use precalculated protein embeddings.\n"," -->\n","Protein embeddings are a machine-friendly method of capturing the protein's structural and functional characteristics, mainly through its sequence. One approach is to train a custom ML model to learn the protein embeddings of the protein sequences in the dataset being used in this notebook. Since this dataset represents proteins using amino-acid sequences which is a standard approach, we can use any publicly available pre-trained protein embedding models to generate the embeddings.\n","\n","There are a variety of protein embedding models. To make data preparation easier, we have used the precalculated protein embeddings created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model in this notebook. The precalculated protein embeddings can be found [here](https://www.kaggle.com/datasets/sergeifironov/t5embeds). We have added this dataset to the notebook along with the dataset made available for the competition.\n","\n","To add this to your enviroment, on the right side panel, click on `Add Data` and search for `t5embeds` (make sure that it's the correct [one](https://www.kaggle.com/datasets/sergeifironov/t5embeds)) and then click on the `+` beside it.\n","\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009086,"end_time":"2023-05-09T08:30:09.140473","exception":false,"start_time":"2023-05-09T08:30:09.131387","status":"completed"},"tags":[]},"source":["# Import the Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:56:52.359142Z","iopub.status.busy":"2023-07-20T15:56:52.358288Z","iopub.status.idle":"2023-07-20T15:57:03.98293Z","shell.execute_reply":"2023-07-20T15:57:03.981665Z","shell.execute_reply.started":"2023-07-20T15:56:52.359095Z"},"papermill":{"duration":9.85331,"end_time":"2023-05-09T08:30:19.002985","exception":false,"start_time":"2023-05-09T08:30:09.149675","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","# Uses only CPU.\n","os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n","# Changes log level.\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import tensorflow as tf\n","\n","\n","DIR_INPUT = Path('/kaggle/input')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008429,"end_time":"2023-05-09T08:30:19.047756","exception":false,"start_time":"2023-05-09T08:30:19.039327","status":"completed"},"tags":[]},"source":["# Load the Dataset"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008388,"end_time":"2023-05-09T08:30:19.065367","exception":false,"start_time":"2023-05-09T08:30:19.056979","status":"completed"},"tags":[]},"source":["First we will load the file `train_terms.tsv` which contains the list of annotated terms (functions) for the proteins. We will extract the labels aka `GO term ID` and create a label dataframe for the protein embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:03.986181Z","iopub.status.busy":"2023-07-20T15:57:03.985354Z","iopub.status.idle":"2023-07-20T15:57:08.76304Z","shell.execute_reply":"2023-07-20T15:57:08.761858Z","shell.execute_reply.started":"2023-07-20T15:57:03.986142Z"},"papermill":{"duration":3.69155,"end_time":"2023-05-09T08:30:22.766144","exception":false,"start_time":"2023-05-09T08:30:19.074594","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_terms = pd.read_csv(\n","    DIR_INPUT / 'cafa-5-protein-function-prediction/Train/train_terms.tsv',\n","    sep='\\t'\n",")\n","print(train_terms.shape)\n","print(round(train_terms.memory_usage().sum() / 1024**3, 2), 'GB')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008358,"end_time":"2023-05-09T08:30:22.783293","exception":false,"start_time":"2023-05-09T08:30:22.774935","status":"completed"},"tags":[]},"source":["`train_terms` dataframe is composed of 3 columns and 5_363_863 entries. We can see all 3 dimensions of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:08.764843Z","iopub.status.busy":"2023-07-20T15:57:08.764439Z","iopub.status.idle":"2023-07-20T15:57:08.793754Z","shell.execute_reply":"2023-07-20T15:57:08.792594Z","shell.execute_reply.started":"2023-07-20T15:57:08.76481Z"},"papermill":{"duration":0.038607,"end_time":"2023-05-09T08:30:22.830633","exception":false,"start_time":"2023-05-09T08:30:22.792026","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_terms.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the protein embeddings\n","\n","\n","We will now load the pre calculated protein embeddings created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model.\n","\n","If the `tfembeds` is not yet on the input data of the notebook, you can add it to your enviromentby clicking on `Add Data` and search for `t5embeds` (make sure that it's the correct [one](https://www.kaggle.com/datasets/sergeifironov/t5embeds) ) and then click on the `+` beside it.\n","\n","The protein embeddings to be used for training are recorded in `train_embeds.npy` and the corresponding protein ids are available in `train_ids.npy`."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009256,"end_time":"2023-05-09T08:30:22.867158","exception":false,"start_time":"2023-05-09T08:30:22.857902","status":"completed"},"tags":[]},"source":["First, we will load the protein ids of the protein embeddings in the train dataset contained in `train_ids.npy` into a numpy array."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:08.796861Z","iopub.status.busy":"2023-07-20T15:57:08.79646Z","iopub.status.idle":"2023-07-20T15:57:08.859095Z","shell.execute_reply":"2023-07-20T15:57:08.857892Z","shell.execute_reply.started":"2023-07-20T15:57:08.796827Z"},"papermill":{"duration":0.067806,"end_time":"2023-05-09T08:30:22.944355","exception":false,"start_time":"2023-05-09T08:30:22.876549","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_protein_ids = np.load(\n","    DIR_INPUT / 't5embeds/train_ids.npy'\n",")\n","print(train_protein_ids.shape)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009498,"end_time":"2023-05-09T08:30:22.963291","exception":false,"start_time":"2023-05-09T08:30:22.953793","status":"completed"},"tags":[]},"source":["The `train_protein_ids` array consists of 142246 protein_ids. Let us print out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:08.860796Z","iopub.status.busy":"2023-07-20T15:57:08.86042Z","iopub.status.idle":"2023-07-20T15:57:08.868825Z","shell.execute_reply":"2023-07-20T15:57:08.867582Z","shell.execute_reply.started":"2023-07-20T15:57:08.860764Z"},"papermill":{"duration":0.019907,"end_time":"2023-05-09T08:30:22.992625","exception":false,"start_time":"2023-05-09T08:30:22.972718","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_protein_ids[:5]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009375,"end_time":"2023-05-09T08:30:23.011402","exception":false,"start_time":"2023-05-09T08:30:23.002027","status":"completed"},"tags":[]},"source":["<!-- Now, we will load`train_embeds.py` which contains the pre-calculated embeddings of the proteins in the train dataset. with protein_ids (`id`s we loaded previously from the **train_ids.npy**) into a numpy array. This array now contains the precalculated embeddings for the protein_ids( Ids we loaded above from **train_ids.npy**) needed for training. -->\n","\n","After loading the files as numpy arrays, we will convert them into Pandas dataframe.\n","\n","Each protein embedding is a vector of length 1024. We create the resulting dataframe such that there are 1024 columns to represent the values in each of the 1024 places in the vector."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:08.870803Z","iopub.status.busy":"2023-07-20T15:57:08.870369Z","iopub.status.idle":"2023-07-20T15:57:20.304079Z","shell.execute_reply":"2023-07-20T15:57:20.302894Z","shell.execute_reply.started":"2023-07-20T15:57:08.870747Z"},"papermill":{"duration":9.719957,"end_time":"2023-05-09T08:30:32.741095","exception":false,"start_time":"2023-05-09T08:30:23.021138","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_embeddings = np.load(\n","    DIR_INPUT / 't5embeds/train_embeds.npy'\n",")\n","\n","# Now lets convert embeddings numpy array(train_embeddings)\n","# into pandas dataframe.\n","column_num = train_embeddings.shape[1]\n","train_df = pd.DataFrame(\n","    train_embeddings,\n","    columns = [\"Column_\" + str(i) for i in range(1, column_num+1)]\n",")\n","# Removes a redundant  object.\n","del train_embeddings\n","print(train_df.shape)\n","print(round(train_df.memory_usage().sum() / 1024**3, 2), 'GB')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00918,"end_time":"2023-05-09T08:30:32.760375","exception":false,"start_time":"2023-05-09T08:30:32.751195","status":"completed"},"tags":[]},"source":["The `train_df` dataframe which contains the embeddings is composed of 1024 columns and 142246 entries. We can see all 1024 dimensions(results will be truncated since column length is too long)  of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:20.306313Z","iopub.status.busy":"2023-07-20T15:57:20.305892Z","iopub.status.idle":"2023-07-20T15:57:20.338892Z","shell.execute_reply":"2023-07-20T15:57:20.33805Z","shell.execute_reply.started":"2023-07-20T15:57:20.306279Z"},"papermill":{"duration":0.036828,"end_time":"2023-05-09T08:30:32.807222","exception":false,"start_time":"2023-05-09T08:30:32.770394","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009208,"end_time":"2023-05-09T08:30:32.825978","exception":false,"start_time":"2023-05-09T08:30:32.81677","status":"completed"},"tags":[]},"source":["# Prepare the dataset\n","\n","Reference: https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009674,"end_time":"2023-05-09T08:30:32.845065","exception":false,"start_time":"2023-05-09T08:30:32.835391","status":"completed"},"tags":[]},"source":["First we will extract all the needed labels(`GO term ID`) from `train_terms.tsv` file. There are more than 40,000 labels. In order to simplify our model, we will choose the most frequent 1500 `GO term ID`s as labels."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009238,"end_time":"2023-05-09T08:30:32.863785","exception":false,"start_time":"2023-05-09T08:30:32.854547","status":"completed"},"tags":[]},"source":["Let's plot the most frequent 100 `GO Term ID`s in `train_terms.tsv`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:20.34101Z","iopub.status.busy":"2023-07-20T15:57:20.34044Z","iopub.status.idle":"2023-07-20T15:57:22.747847Z","shell.execute_reply":"2023-07-20T15:57:22.746662Z","shell.execute_reply.started":"2023-07-20T15:57:20.340976Z"},"papermill":{"duration":1.592489,"end_time":"2023-05-09T08:30:34.465912","exception":false,"start_time":"2023-05-09T08:30:32.873423","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Select first 100 values for plotting.\n","plot_df = train_terms['term'].value_counts().iloc[:100]\n","\n","figure, axis = plt.subplots(1, 1, figsize=(12, 6))\n","\n","bp = sns.barplot(ax=axis, x=np.array(plot_df.index), y=plot_df.values)\n","bp.set_xticklabels(bp.get_xticklabels(), rotation=90, size = 6)\n","axis.set_title('Top 100 frequent GO term IDs')\n","bp.set_xlabel('GO term IDs', fontsize = 12)\n","bp.set_ylabel('Count', fontsize = 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010458,"end_time":"2023-05-09T08:30:34.487707","exception":false,"start_time":"2023-05-09T08:30:34.477249","status":"completed"},"tags":[]},"source":["We will now save the first 1500 most frequent GO term Ids into a list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:22.74951Z","iopub.status.busy":"2023-07-20T15:57:22.74917Z","iopub.status.idle":"2023-07-20T15:57:23.829398Z","shell.execute_reply":"2023-07-20T15:57:23.828416Z","shell.execute_reply.started":"2023-07-20T15:57:22.74948Z"},"papermill":{"duration":0.523976,"end_time":"2023-05-09T08:30:35.021974","exception":false,"start_time":"2023-05-09T08:30:34.497998","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Set the limit for label.\n","num_of_labels = 1500\n","\n","# Take value counts in descending order\n","# and fetch first 1500 `GO term ID` as labels\n","labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009833,"end_time":"2023-05-09T08:30:35.042088","exception":false,"start_time":"2023-05-09T08:30:35.032255","status":"completed"},"tags":[]},"source":["Next, we will create a new dataframe by filtering the train terms with the selected `GO Term ID`s."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:23.833426Z","iopub.status.busy":"2023-07-20T15:57:23.833069Z","iopub.status.idle":"2023-07-20T15:57:24.852213Z","shell.execute_reply":"2023-07-20T15:57:24.851059Z","shell.execute_reply.started":"2023-07-20T15:57:23.833397Z"},"papermill":{"duration":0.668657,"end_time":"2023-05-09T08:30:35.720953","exception":false,"start_time":"2023-05-09T08:30:35.052296","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Fetch the train_terms data for the relevant labels only\n","train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]\n","del train_terms"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009797,"end_time":"2023-05-09T08:30:35.741328","exception":false,"start_time":"2023-05-09T08:30:35.731531","status":"completed"},"tags":[]},"source":["Let us plot the aspect values in the new **train_terms_updated** dataframe using a pie chart."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:24.854641Z","iopub.status.busy":"2023-07-20T15:57:24.853937Z","iopub.status.idle":"2023-07-20T15:57:25.721539Z","shell.execute_reply":"2023-07-20T15:57:25.720046Z","shell.execute_reply.started":"2023-07-20T15:57:24.854582Z"},"papermill":{"duration":0.419624,"end_time":"2023-05-09T08:30:36.171193","exception":false,"start_time":"2023-05-09T08:30:35.751569","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["pie_df = train_terms_updated['aspect'].value_counts()\n","palette_color = sns.color_palette('bright')\n","plt.pie(pie_df.values, labels=np.array(pie_df.index), colors=palette_color, autopct='%.0f%%')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016642,"end_time":"2023-05-09T08:30:36.204943","exception":false,"start_time":"2023-05-09T08:30:36.188301","status":"completed"},"tags":[]},"source":["As you can see, majority of the `GO term Id`s have BPO(Biological Process Ontology) as their aspect."]},{"cell_type":"markdown","metadata":{},"source":["Since this is a multi label classification problem, in the labels array we will denote the presence or absence of each Go Term Id for a protein id using a 1 or 0.\n","First, we will create a numpy array `train_labels` of required size for the labels. To update the `train_labels` array with the appropriate values, we will loop through the label list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T15:57:25.724547Z","iopub.status.busy":"2023-07-20T15:57:25.723347Z","iopub.status.idle":"2023-07-20T16:18:18.788945Z","shell.execute_reply":"2023-07-20T16:18:18.787693Z","shell.execute_reply.started":"2023-07-20T15:57:25.724499Z"},"papermill":{"duration":495.729474,"end_time":"2023-05-09T08:38:51.951408","exception":false,"start_time":"2023-05-09T08:30:36.221934","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Create an empty dataframe of required size for storing the labels,\n","# i.e, train_size x num_of_labels (142246 x 1500)\n","train_size = train_protein_ids.shape[0] # len(X)\n","train_labels = np.zeros((train_size ,num_of_labels))\n","\n","# Convert from numpy to pandas series for better handling\n","series_train_protein_ids = pd.Series(train_protein_ids)\n","\n","# Loop through each label\n","for i in range(num_of_labels):\n","    # For each label, fetch the corresponding train_terms data\n","    n_train_terms = train_terms_updated[\n","        train_terms_updated['term'] ==  labels[i]\n","    ]\n","    \n","    # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n","    label_related_proteins = n_train_terms['EntryID'].unique()\n","    \n","    # In the series_train_protein_ids pandas series, if a protein is related\n","    # to the current label, then mark it as 1, else 0.\n","    # Replace the ith column of train_Y with with that pandas series.\n","    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n","    \n","# Convert train_Y numpy into pandas dataframe\n","labels_df = pd.DataFrame(data = train_labels, columns = labels)\n","labels_df = labels_df.astype('int32')\n","print(labels_df.shape)\n","print(round(labels_df.memory_usage().sum() / 1024**3, 2), 'GB')\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010097,"end_time":"2023-05-09T08:38:51.971947","exception":false,"start_time":"2023-05-09T08:38:51.96185","status":"completed"},"tags":[]},"source":["The final labels dataframe (`label_df`) is composed of 1500 columns and 142_246 entries. We can see all 1500 dimensions(results will be truncated since the number of columns is big) of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:18:18.79106Z","iopub.status.busy":"2023-07-20T16:18:18.790677Z","iopub.status.idle":"2023-07-20T16:18:18.829129Z","shell.execute_reply":"2023-07-20T16:18:18.827778Z","shell.execute_reply.started":"2023-07-20T16:18:18.791028Z"},"papermill":{"duration":0.048128,"end_time":"2023-05-09T08:38:52.031041","exception":false,"start_time":"2023-05-09T08:38:51.982913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["labels_df.head()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010523,"end_time":"2023-05-09T08:38:52.052433","exception":false,"start_time":"2023-05-09T08:38:52.04191","status":"completed"},"tags":[]},"source":["# Training\n","\n","Next, we will use Tensorflow to train a Deep Neural Network with the protein embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:18:18.832025Z","iopub.status.busy":"2023-07-20T16:18:18.831219Z","iopub.status.idle":"2023-07-20T16:20:46.045383Z","shell.execute_reply":"2023-07-20T16:20:46.043988Z","shell.execute_reply.started":"2023-07-20T16:18:18.831977Z"},"papermill":{"duration":128.96621,"end_time":"2023-05-09T08:41:01.029422","exception":false,"start_time":"2023-05-09T08:38:52.063212","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["INPUT_SHAPE = [train_df.shape[1]]\n","BATCH_SIZE = 5120\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),    \n","    tf.keras.layers.Dense(units=512, activation='relu'),\n","    tf.keras.layers.Dense(units=512, activation='relu'),\n","    tf.keras.layers.Dense(units=512, activation='relu'),\n","    tf.keras.layers.Dense(units=num_of_labels,activation='sigmoid')\n","])\n","\n","\n","# Compile model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","    loss='binary_crossentropy',\n","    metrics=['binary_accuracy', tf.keras.metrics.AUC()],\n",")\n","\n","history = model.fit(\n","    train_df, labels_df,\n","    batch_size=BATCH_SIZE,\n","    epochs=5\n",")\n","# Removes redundant objects.\n","del train_df\n","del labels_df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.021665,"end_time":"2023-05-09T08:41:01.780867","exception":false,"start_time":"2023-05-09T08:41:01.759202","status":"completed"},"tags":[]},"source":["# Submission"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.02075,"end_time":"2023-05-09T08:41:01.82296","exception":false,"start_time":"2023-05-09T08:41:01.80221","status":"completed"},"tags":[]},"source":["For submission we will use the protein embeddings of the test data created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:22:45.081907Z","iopub.status.busy":"2023-07-20T16:22:45.081382Z","iopub.status.idle":"2023-07-20T16:22:55.750604Z","shell.execute_reply":"2023-07-20T16:22:55.749436Z","shell.execute_reply.started":"2023-07-20T16:22:45.081872Z"},"papermill":{"duration":10.290827,"end_time":"2023-05-09T08:41:12.134919","exception":false,"start_time":"2023-05-09T08:41:01.844092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_embeddings = np.load(\n","    DIR_INPUT / 't5embeds/test_embeds.npy'\n",")\n","\n","# Convert test_embeddings to dataframe\n","column_num = test_embeddings.shape[1]\n","test_df = pd.DataFrame(test_embeddings, columns = [\"Column_\" + str(i) for i in range(1, column_num+1)])\n","print(test_df.shape)\n","print(test_df.memory_usage().sum() / 1024**3, 'GB')\n","print(round(test_df.memory_usage().sum() / 1024**3, 2), 'GB')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.020857,"end_time":"2023-05-09T08:41:12.17776","exception":false,"start_time":"2023-05-09T08:41:12.156903","status":"completed"},"tags":[]},"source":["The `test_df` is composed of 1024 columns and 141865 entries. We can see all 1024 dimensions(results will be truncated since column length is too long) of our dataset by printing out the first 5 entries using the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:23:02.160382Z","iopub.status.busy":"2023-07-20T16:23:02.159965Z","iopub.status.idle":"2023-07-20T16:23:02.195268Z","shell.execute_reply":"2023-07-20T16:23:02.194011Z","shell.execute_reply.started":"2023-07-20T16:23:02.160351Z"},"papermill":{"duration":0.050123,"end_time":"2023-05-09T08:41:12.248732","exception":false,"start_time":"2023-05-09T08:41:12.198609","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["We will now use the model to make predictions on the test embeddings. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:23:06.168709Z","iopub.status.busy":"2023-07-20T16:23:06.168269Z","iopub.status.idle":"2023-07-20T16:23:32.380471Z","shell.execute_reply":"2023-07-20T16:23:32.379303Z","shell.execute_reply.started":"2023-07-20T16:23:06.168661Z"},"papermill":{"duration":663.907351,"end_time":"2023-05-09T08:52:16.178461","exception":false,"start_time":"2023-05-09T08:41:12.27111","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["predictions = model.predict(test_df)\n","# Removes a redundant  object.\n","del test_df\n","print(f'`predictions` shape: {predictions.shape}')\n","# Rounding of probabilities\n","# according to the requirements of the competition rules.\n","predictions = predictions.round(3)\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:23:34.525978Z","iopub.status.busy":"2023-07-20T16:23:34.524611Z","iopub.status.idle":"2023-07-20T16:23:34.586073Z","shell.execute_reply":"2023-07-20T16:23:34.58442Z","shell.execute_reply.started":"2023-07-20T16:23:34.525934Z"},"trusted":true},"outputs":[],"source":["# Loads protein IDs from embedded test data.\n","test_protein_ids = np.load(\n","    DIR_INPUT / 't5embeds/test_ids.npy'\n",")\n","assert test_protein_ids.shape[0] == predictions.shape[0], (\n","    'Number protein ids in embedded list must be equal to proteins number in predictions.'\n",")\n","test_protein_ids"]},{"cell_type":"markdown","metadata":{},"source":["Creates from `predictions` `submission.tsv` file in streaming writing manner."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-20T16:23:43.271384Z","iopub.status.busy":"2023-07-20T16:23:43.270966Z","iopub.status.idle":"2023-07-20T16:41:30.073331Z","shell.execute_reply":"2023-07-20T16:41:30.071303Z","shell.execute_reply.started":"2023-07-20T16:23:43.271353Z"},"papermill":{"duration":0.063739,"end_time":"2023-05-09T08:52:16.292974","exception":false,"start_time":"2023-05-09T08:52:16.229235","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["submission_chunk_df = pd.DataFrame(\n","    columns = ['Protein Id', 'GO Term Id','Prediction']\n",")\n","for prot_idx, prot_id in enumerate(test_protein_ids, 0):\n","    prot_chunk = [prot_id] * predictions.shape[1]\n","\n","    submission_chunk_df['Protein Id'] = [prot_id] * predictions.shape[1]\n","    submission_chunk_df['GO Term Id'] = labels\n","    submission_chunk_df['Prediction'] = predictions[prot_idx]\n","\n","    submission_chunk_df.to_csv(\n","        '/kaggle/working/submission.tsv',\n","        header=False,\n","        index=False,\n","        mode='a',\n","        sep='\\t'\n","    )\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
